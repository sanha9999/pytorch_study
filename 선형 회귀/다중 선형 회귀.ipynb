{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22fa68ac910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1) # 다시 돌려도 랜덤값이 변하지 않게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = torch.FloatTensor([[73], [93], [89], [96],[73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[80],[100],[70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/20000, w1 : 0.294, w2 : 0.294. w3 : 0.290, b : 0.003 cost : 29661.801\n",
      "epoch : 500/20000, w1 : 0.724, w2 : 0.691. w3 : 0.611, b : 0.008 cost : 4.852\n",
      "epoch : 1000/20000, w1 : 0.766, w2 : 0.700. w3 : 0.559, b : 0.009 cost : 3.942\n",
      "epoch : 1500/20000, w1 : 0.805, w2 : 0.707. w3 : 0.513, b : 0.009 cost : 3.220\n",
      "epoch : 2000/20000, w1 : 0.840, w2 : 0.711. w3 : 0.473, b : 0.009 cost : 2.643\n",
      "epoch : 2500/20000, w1 : 0.873, w2 : 0.713. w3 : 0.438, b : 0.009 cost : 2.181\n",
      "epoch : 3000/20000, w1 : 0.903, w2 : 0.713. w3 : 0.407, b : 0.010 cost : 1.807\n",
      "epoch : 3500/20000, w1 : 0.931, w2 : 0.713. w3 : 0.380, b : 0.010 cost : 1.504\n",
      "epoch : 4000/20000, w1 : 0.956, w2 : 0.712. w3 : 0.355, b : 0.010 cost : 1.258\n",
      "epoch : 4500/20000, w1 : 0.979, w2 : 0.710. w3 : 0.334, b : 0.010 cost : 1.056\n",
      "epoch : 5000/20000, w1 : 1.000, w2 : 0.707. w3 : 0.315, b : 0.010 cost : 0.891\n",
      "epoch : 5500/20000, w1 : 1.020, w2 : 0.705. w3 : 0.298, b : 0.010 cost : 0.756\n",
      "epoch : 6000/20000, w1 : 1.038, w2 : 0.702. w3 : 0.282, b : 0.010 cost : 0.645\n",
      "epoch : 6500/20000, w1 : 1.054, w2 : 0.699. w3 : 0.269, b : 0.010 cost : 0.553\n",
      "epoch : 7000/20000, w1 : 1.069, w2 : 0.696. w3 : 0.257, b : 0.009 cost : 0.477\n",
      "epoch : 7500/20000, w1 : 1.082, w2 : 0.694. w3 : 0.246, b : 0.009 cost : 0.414\n",
      "epoch : 8000/20000, w1 : 1.095, w2 : 0.691. w3 : 0.236, b : 0.009 cost : 0.363\n",
      "epoch : 8500/20000, w1 : 1.106, w2 : 0.688. w3 : 0.227, b : 0.009 cost : 0.320\n",
      "epoch : 9000/20000, w1 : 1.117, w2 : 0.685. w3 : 0.219, b : 0.009 cost : 0.284\n",
      "epoch : 9500/20000, w1 : 1.126, w2 : 0.683. w3 : 0.212, b : 0.008 cost : 0.255\n",
      "epoch : 10000/20000, w1 : 1.135, w2 : 0.681. w3 : 0.206, b : 0.008 cost : 0.230\n",
      "epoch : 10500/20000, w1 : 1.143, w2 : 0.678. w3 : 0.200, b : 0.008 cost : 0.210\n",
      "epoch : 11000/20000, w1 : 1.150, w2 : 0.676. w3 : 0.195, b : 0.008 cost : 0.193\n",
      "epoch : 11500/20000, w1 : 1.157, w2 : 0.674. w3 : 0.190, b : 0.008 cost : 0.179\n",
      "epoch : 12000/20000, w1 : 1.163, w2 : 0.673. w3 : 0.186, b : 0.007 cost : 0.167\n",
      "epoch : 12500/20000, w1 : 1.168, w2 : 0.671. w3 : 0.182, b : 0.007 cost : 0.158\n",
      "epoch : 13000/20000, w1 : 1.173, w2 : 0.669. w3 : 0.178, b : 0.007 cost : 0.150\n",
      "epoch : 13500/20000, w1 : 1.178, w2 : 0.668. w3 : 0.175, b : 0.006 cost : 0.143\n",
      "epoch : 14000/20000, w1 : 1.182, w2 : 0.667. w3 : 0.172, b : 0.006 cost : 0.137\n",
      "epoch : 14500/20000, w1 : 1.186, w2 : 0.665. w3 : 0.170, b : 0.006 cost : 0.133\n",
      "epoch : 15000/20000, w1 : 1.189, w2 : 0.664. w3 : 0.167, b : 0.005 cost : 0.129\n",
      "epoch : 15500/20000, w1 : 1.193, w2 : 0.663. w3 : 0.165, b : 0.005 cost : 0.126\n",
      "epoch : 16000/20000, w1 : 1.195, w2 : 0.662. w3 : 0.163, b : 0.005 cost : 0.123\n",
      "epoch : 16500/20000, w1 : 1.198, w2 : 0.661. w3 : 0.161, b : 0.005 cost : 0.121\n",
      "epoch : 17000/20000, w1 : 1.201, w2 : 0.660. w3 : 0.160, b : 0.004 cost : 0.119\n",
      "epoch : 17500/20000, w1 : 1.203, w2 : 0.660. w3 : 0.158, b : 0.004 cost : 0.117\n",
      "epoch : 18000/20000, w1 : 1.205, w2 : 0.659. w3 : 0.157, b : 0.004 cost : 0.116\n",
      "epoch : 18500/20000, w1 : 1.207, w2 : 0.658. w3 : 0.156, b : 0.003 cost : 0.115\n",
      "epoch : 19000/20000, w1 : 1.208, w2 : 0.658. w3 : 0.155, b : 0.003 cost : 0.114\n",
      "epoch : 19500/20000, w1 : 1.210, w2 : 0.657. w3 : 0.154, b : 0.003 cost : 0.113\n",
      "epoch : 20000/20000, w1 : 1.211, w2 : 0.657. w3 : 0.153, b : 0.002 cost : 0.113\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "epochs=20000\n",
    "for epoch in range(epochs+1):\n",
    "    hp = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    cost = torch.mean((hp - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(\"epoch : {}/{}, w1 : {:.3f}, w2 : {:.3f}. w3 : {:.3f}, b : {:.3f} cost : {:.3f}\".format(epoch, epochs,\n",
    "                                                                                  w1.item(), w2.item(), w3.item(), b.item(), cost.item()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 연산을 고려하여 파이토치로 구현\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 80],\n",
    "                            [96, 98, 100],\n",
    "                            [73,66, 70]])\n",
    "y_train = torch.FloatTensor(([[152], [185], [180], [196], [142]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 hypothesis: tensor([152.4085, 184.6335, 179.7753, 195.9034, 142.4502]) Cost: 0.112729\n",
      "Epoch 1000/20000 hypothesis: tensor([152.3978, 184.6360, 179.7861, 195.8934, 142.4593]) Cost: 0.111764\n",
      "Epoch 2000/20000 hypothesis: tensor([152.3888, 184.6380, 179.7951, 195.8852, 142.4669]) Cost: 0.111084\n",
      "Epoch 3000/20000 hypothesis: tensor([152.3813, 184.6397, 179.8025, 195.8783, 142.4732]) Cost: 0.110592\n",
      "Epoch 4000/20000 hypothesis: tensor([152.3750, 184.6411, 179.8087, 195.8727, 142.4785]) Cost: 0.110233\n",
      "Epoch 5000/20000 hypothesis: tensor([152.3698, 184.6424, 179.8139, 195.8680, 142.4828]) Cost: 0.109974\n",
      "Epoch 6000/20000 hypothesis: tensor([152.3654, 184.6434, 179.8181, 195.8640, 142.4864]) Cost: 0.109777\n",
      "Epoch 7000/20000 hypothesis: tensor([152.3618, 184.6442, 179.8216, 195.8608, 142.4894]) Cost: 0.109629\n",
      "Epoch 8000/20000 hypothesis: tensor([152.3587, 184.6450, 179.8246, 195.8582, 142.4918]) Cost: 0.109506\n",
      "Epoch 9000/20000 hypothesis: tensor([152.3562, 184.6456, 179.8271, 195.8559, 142.4939]) Cost: 0.109405\n",
      "Epoch 10000/20000 hypothesis: tensor([152.3539, 184.6462, 179.8291, 195.8541, 142.4956]) Cost: 0.109317\n",
      "Epoch 11000/20000 hypothesis: tensor([152.3522, 184.6466, 179.8308, 195.8526, 142.4969]) Cost: 0.109243\n",
      "Epoch 12000/20000 hypothesis: tensor([152.3507, 184.6470, 179.8323, 195.8514, 142.4981]) Cost: 0.109178\n",
      "Epoch 13000/20000 hypothesis: tensor([152.3494, 184.6472, 179.8334, 195.8504, 142.4989]) Cost: 0.109115\n",
      "Epoch 14000/20000 hypothesis: tensor([152.3483, 184.6476, 179.8345, 195.8495, 142.4997]) Cost: 0.109052\n",
      "Epoch 15000/20000 hypothesis: tensor([152.3474, 184.6478, 179.8354, 195.8488, 142.5003]) Cost: 0.108994\n",
      "Epoch 16000/20000 hypothesis: tensor([152.3466, 184.6480, 179.8360, 195.8483, 142.5007]) Cost: 0.108943\n",
      "Epoch 17000/20000 hypothesis: tensor([152.3460, 184.6482, 179.8365, 195.8479, 142.5011]) Cost: 0.108884\n",
      "Epoch 18000/20000 hypothesis: tensor([152.3454, 184.6483, 179.8370, 195.8476, 142.5014]) Cost: 0.108834\n",
      "Epoch 19000/20000 hypothesis: tensor([152.3448, 184.6485, 179.8375, 195.8472, 142.5017]) Cost: 0.108782\n",
      "Epoch 20000/20000 hypothesis: tensor([152.3443, 184.6487, 179.8379, 195.8469, 142.5020]) Cost: 0.108729\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "epochs = 20000\n",
    "for epoch in range(epochs+1):\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W)+b\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "       print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
